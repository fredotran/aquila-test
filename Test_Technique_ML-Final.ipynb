{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Quel(le) data scientist êtes-vous ?\n",
    "## Contexte de l’analyse\n",
    "\n",
    "Elu métier le plus sexy par la Harvard Business Review en octobre 2012, le data scientist représente un profil rare qui exige de nombreuses compétences.\n",
    "\n",
    "A partir d'un dataset Aquila, vous réaliserez :\n",
    "- un clustering non supervisé afin d'identifier 2 groupes de profils techniques distinctes\n",
    "- une prédiction des profils dont le métier n'est pas labellisé\n",
    "\n",
    "\n",
    "## Données\n",
    "data.csv contient 6 variables : \n",
    "    - 'Entreprise' correspond à une liste d'entreprises fictive\n",
    "    - 'Metier' correspond au métier parmi data scientist, lead data scientist, data engineer et data architecte\n",
    "    - 'Technologies' correspond aux compétences maîtrisées par le profil\n",
    "    - 'Diplome' correspond à son niveau scolaire (Bac, Master, PhD,...)\n",
    "    - 'Experience' correspond au nombre d'années d'expériences\n",
    "    - 'Ville' correspond au lieu de travail\n",
    "    \n",
    "\n",
    "\n",
    "## Répondez aux questions \n",
    "\n",
    "Bonne chance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des libraries classique (numpy, pandas, ...)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn as sk\n",
    "import seaborn as sb\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking python version\n",
    "import sys\n",
    "print(f\"Python version : {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Importer le tableau de données dans un dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du dataframe \"data.csv\"\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Combien y a t-il d'observations dans ce dataset? Y a t-il des valeurs manquantes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse :**\n",
    "\n",
    "Le dataset est constitué de **6 colonnes différentes**, nous récupérons le nom de ces différentes colonnes puis les ajoutons dans une liste appelée `categories`. Ensuite nous allons pouvoir étudier chaque valeur manquante de ce dataset en utilisant des méthodes implémentées directement dans la librairie `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column names\n",
    "categories = [categories for categories in df.columns]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par habitude, nous allons utiliser `X_data` pour nommer les listes de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data values using the features we defined before\n",
    "X_data = df[categories]\n",
    "\n",
    "# visualize the shape of the data we've collected\n",
    "print(f\"Data shape : {X_data.shape} \\nNumber of rows : {X_data.shape[0]} \\nNumber of columns : {X_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset est constitué de **9582 lignes** (instances/observations) et de **6 colonnes** en référence aux catégories que nous avions extraites au préalable.\n",
    "Désormais, sachant que nous connaissons les différentes catégories et le nombre de données disponibles, nous allons pouvoir creuser un peu plus en détails, en commençant par passer en revue **les premières données du dataset**. Pour ce faire, nous allons utiliser la méthode **head()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quick review of the data\n",
    "X_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous regardons ensuite le nombre de valeurs manquantes dans notre dataset en utilisant les méthodes **isna()** ou **isnull()** directement disponibles à travers `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the na values in our dataset and summing them \n",
    "na_values = X_data.isna()\n",
    "number_na = na_values.sum()\n",
    "total_number_na = number_na.sum()\n",
    "\n",
    "print(f\"Number of missing values per categories :\\n\")\n",
    "print(f\"{number_na} \\n\\nTotal of missing values in all categories : {total_number_na}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer qu'il y a des valeurs manquantes (`NaN`) dans les catégories **`Experience`**, **`Metier`** et **`Entreprise`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Réaliser l'imputation des valeurs manquantes pour la variable \"Experience\" avec : \n",
    "- la valeur médiane pour les data scientists\n",
    "- la valeur moyenne pour les data engineers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse :**\n",
    "\n",
    "Il y a plusieurs façons de remédier aux problèmes des valeurs manquantes, dans cet exercice nous allons utiliser les méthodes fournies directement dans `pandas`, mais nous pouvons aussi utiliser celle de `scikit-learn` avec le module `SimpleImputer`.  \n",
    "\n",
    "La méthode **fillna()** de `pandas` nous permet de \"impute\" de nouvelles valeurs sur les valeurs manquantes, tout en choisissant la manière que l'on veut **i.e.** en remplaçant par la médiane des valeurs ou la moyenne (au choix). D'après nos observations, seules les colonnes **`Experience`**, **`Metier`** et **`Entreprise`** possèdent des valeurs manquantes. Sachant que la colonne **`Experience`** est la seule contenant des *valeurs numériques* nous pouvons donc \"imputer\" les valeurs médianes et moyennes seulement dans cette colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy to avoid changing original data \n",
    "X_data_copy = X_data.copy()\n",
    "# we get the data type of the values in the category 'Experience'\n",
    "X_data_copy['Experience'].dtypes\n",
    "# output dtype = object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En regardant attentivement les données dans la catégorie **`Experience`**, nous avons remarqué que les valeurs décimales sont sous un format à virgule et que le type de données de la colonne est `object`.  \n",
    "Il est donc nécessaire de transformer le type de données de la colonne **`Experience`** en `float` pour pouvoir calculer la valeur moyenne et la valeur médiane, mais aussi de remplacer **TOUTES** les virgules par des points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the object types to float type\n",
    "X_data_copy['Experience'] = X_data_copy['Experience'].str.replace(',', '.').astype(float)\n",
    "X_data_copy['Experience'].dtypes\n",
    "# output dtype = float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons ensuite la méthode **loc()** pour imposer nos conditions et localiser les différents métiers correspondant à la colonne **`Experience`** qui nous intéresse. Finalement, nous allons imputer les valeurs moyennes et médianes correspondantes à ces métiers dans la colonne **`Experience`**. \n",
    "\n",
    "**A noter que** : si nous nous trompons dans l'orthographe du métier dans la condition (cf. mask), `pandas` ne prendra pas en compte les instructions que l'on confie ensuite **i.e** : les lignes contenant des valeurs manquantes ne seront pas remplies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we'll use masks to respect the conditions, here we want to put median value experience for data scientist\n",
    "mask1 = (X_data_copy['Metier'] == 'Data scientist')\n",
    "X_data_copy.loc[mask1, 'Experience'] = X_data_copy.loc[mask1, 'Experience'].fillna(X_data_copy.loc[mask1, 'Experience'].median())\n",
    "\n",
    "# we'll use masks to respect the conditions, here we want to put median value experience for lead data scientist\n",
    "mask2 = (X_data_copy['Metier'] == 'Lead data scientist')\n",
    "X_data_copy.loc[mask2, 'Experience'] = X_data_copy.loc[mask2, 'Experience'].fillna(X_data_copy.loc[mask2, 'Experience'].median())\n",
    "\n",
    "# we'll use masks to respect the conditions, here we want to put mean value experience for data engineer\n",
    "mask3 = (X_data_copy['Metier'] == 'Data engineer')\n",
    "X_data_copy.loc[mask3, 'Experience'] = X_data_copy.loc[mask3, 'Experience'].fillna(X_data_copy.loc[mask3, 'Experience'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the results by verifying is there are still any missing values\n",
    "print(f\"Are there any missing values for data scientist? : {X_data_copy.loc[mask1, 'Experience'].isna().any()}\")\n",
    "print(f\"Are there any missing values for lead data scientist? : {X_data_copy.loc[mask2, 'Experience'].isna().any()}\")\n",
    "print(f\"Are there any missing values for data engineer? : {X_data_copy.loc[mask3, 'Experience'].isna().any()}\")\n",
    "\n",
    "# we can check the X_data values to be sure\n",
    "X_data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer les statistiques de la colonne `Experience` en utilisant la méthode **describe()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the values back in X_data\n",
    "X_data = X_data_copy\n",
    "# generate descriptive statistics of the 'Experience' column which is the only column with float values\n",
    "X_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir des données statistisques que nous avons récupéré et les codes écrits précédents, nous pouvons répondre à la question suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Combien d'années d'expériences ont, en moyenne, chacun des profils : le data scientist, le lead data scientist et le data engineer en moyenne?\n",
    "\n",
    "**Réponse :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mask for data architecte \n",
    "mask4 = (X_data['Metier'] == 'Data architecte')\n",
    "\n",
    "# mean values calculations\n",
    "mean_values_data_scientist = X_data.loc[mask1, 'Experience'].mean()\n",
    "mean_values_lead_data_scientist = X_data.loc[mask2, 'Experience'].mean()\n",
    "mean_values_data_engineer = X_data.loc[mask3, 'Experience'].mean()\n",
    "mean_values_data_architecte = X_data.loc[mask4, 'Experience'].mean()\n",
    "\n",
    "print(f'Mean value for years of experience working (data scientist) : {mean_values_data_scientist:.2f} years') \n",
    "print(f'Mean value for years of experience working (Lead data scientist) : {mean_values_lead_data_scientist:.2f} years') \n",
    "print(f'Mean value for years of experience working (data engineer) : {mean_values_data_engineer:.2f} years') \n",
    "print(f'Mean value for years of experience working (data architecte) : {mean_values_data_architecte:.2f} years') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean value for all :\n",
    "mean_value = 1/4*(mean_values_data_scientist + mean_values_lead_data_scientist \n",
    "                                             + mean_values_data_engineer \n",
    "                                             + mean_values_data_architecte)\n",
    "\n",
    "print(f'Mean value for years of experience working for all types of jobs : {mean_value:.2f} years') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur moyenne d'expérience au travail d'un data architecte **est très suspecte**, nous vérifions donc s'il manque des valeurs dans le dataset de cette catégorie de métier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Are there any missing values for data architecte? : {X_data.loc[mask4, 'Experience'].isna().any()}\")\n",
    "print(f\"There are {X_data.loc[mask4, 'Experience'].isna().sum()} missing year of working experience values for the data architecte job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons qu'il y a **23 valeurs manquantes** pour le métier de **data architecte**, néanmoins il n'y a aucune consigne indiquée sur l'imputation de ces valeurs manquantes. Nous pouvons donc décider oui ou non d'imputer une moyenne/médiane des valeurs totales pour ces valeurs manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Faire la représentation graphique de votre choix afin de comparer le nombre moyen d'années d'expériences pour chaque métier\n",
    "\n",
    "**Réponse :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean value of year experience for each jobs\n",
    "f, ax = plt.subplots(figsize=(11, 7))\n",
    "sb.set_color_codes(\"muted\")\n",
    "sb.barplot(x='Metier', y='Experience', data=X_data, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Transformer la variable continue 'Experience' en une nouvelle variable catégorielle 'Exp_label' à 4 modalités: débutant, confirmé, avancé et expert\n",
    "- Veuillez expliquer votre choix du règle de transformation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse :** \n",
    "\n",
    "Il y a différentes façons de transformer des variables **continues** en **catégorielles**, parmi elles, il y a les techniques classiques dites de **\"label encoding\"** ou de **\"one-hot encoding\"**. Pour la méthode de **\"label encoding\"** les variables continues doivent être *ordonnées* c'est à dire qu'elles doivent respecter un ordre ou une hiérarchie entre elles. \n",
    "\n",
    "Prenons l'exemple d'une variable **`Brossage de dents après le petit-déj`** contenant des valeurs ordonnées : `Tous les jours` ,`Jamais` ,`Rarement`, `Parfois`; nous observons donc bien une hiérarchie entre les valeurs possibles de cette variable. Nous pouvons, dès lors, appliquer la méthode de **\"label encoding\"** pour transformer la variable continue en variable catégorielle à 4 modalités (classées par ordre hiérarchique): \n",
    "\n",
    "* `Jamais` (0) <  `Rarement` (1) < `Parfois` (2) < `Tous les jours` (3).\n",
    "\n",
    "Dans notre cas d'étude, nous allons transformer notre variable continue **`Experience`** en variable catégorielle **`Exp_label`** à 4 modalités en utilisant une méthode implémentée dans `pandas` directement.  \n",
    "Nous allons encoder la nouvelle variable catégorielle comme suit (classées par ordre hiérarchique) : \n",
    "\n",
    "* `Débutant` (0) <  `Confirmé` (1) < `Avancé` (2) < `Expert` (3).\n",
    "\n",
    "Il existe plusieurs façons de faire du **\"label encoding\"**, en utilisant le module `LabelEncoder` dans `scikit-learn`  ou bien les méthodes de catégorisation de `pandas`. Dans ce cas de figure, nous allons utiliser les méthodes de `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make copy to avoid changing original data \n",
    "label_X_data = X_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the new column using cut() method from pandas\n",
    "label_X_data['Exp_label'] = pd.cut(label_X_data['Experience'], 4, labels=['Débutant', 'Confirmé', 'Avancé', 'Expert'])\n",
    "# check the result\n",
    "label_X_data['Exp_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overall view of the new transformed dataset \n",
    "label_X_data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc bien réussi à créer une colonne **`Exp_label`** avec des valeurs de variables catégorielles respectant la hiérarchie suivante : \n",
    "* `Débutant` (0) <  `Confirmé` (1) < `Avancé` (2) < `Expert` (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Quelles sont les 5 technologies les plus utilisées? Faites un graphique\n",
    "\n",
    "\n",
    "**Réponse :**\n",
    "\n",
    "Nous allons d'abord prétaiter les données contenues dans la colonne **`Technologies`**, nous transformons la colonne en `list` puis faisons en sorte de rassembler tous les éléments de cette liste ensemble dans un **dictionnaire**. A partir de ce **dictionnaire**, nous allons compter le nombre de répétitions des différentes technologies, et ainsi pouvoir déterminer lesquelles sont les plus utilisées. Pour ce faire, nous allons utiliser la méthode **count()**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make copy to avoid changing original data \n",
    "lst_X_data = label_X_data.copy()\n",
    "# for checking\n",
    "#print(lst_X_data['Technologies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract\n",
    "def extract_slash(lst): \n",
    "    \"\"\" This function split the elements with a slash suffix\"\"\"\n",
    "    res = [] \n",
    "    for element in lst: \n",
    "        sub_list = element.split('/')         \n",
    "        res.append(sub_list)       \n",
    "    return(res) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous choisissons d'utiliser une méthode de comptage à l'aide d'un **dictionnaire Python**, ce n'est pas la meilleure méthode car elle est assez lente (quelques secondes d'attente lors d'un run). Néanmoins, elle reste très efficace et permet de classer **par ordre décroissant** le nombre d'apparitions des technologies à travers le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into a list\n",
    "raw_tech_list = lst_X_data['Technologies'].tolist()\n",
    "\n",
    "# extract elements with a slash suffix and stored them in sublists\n",
    "raw_tech_list = extract_slash(raw_tech_list)\n",
    "tech_list = []\n",
    "for elem in raw_tech_list:\n",
    "    tech_list.extend(elem)\n",
    "\n",
    "# checking our dict before processing\n",
    "tech_dict_no_processed = {i:tech_list.count(i) for i in tech_list}\n",
    "print(tech_dict_no_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons qu'il y a des soucis dans les noms de certaines technologies. En effet par exemple pour `scikit-learn` : il y a deux écritures différentes dans notre colonne **`Technologies`**, avec le **s** majuscule ou mininuscule; nous devons donc uniformiser et corriger toutes les erreurs dans tous les mots-clés nommant les technologies, avant de pouvoir faire notre compte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all the redundants words with only 1 version\n",
    "def get_processed(tech_list):\n",
    "    \"\"\" Processing error names \"\"\"\n",
    "    tech_list_processed = []\n",
    "    for tech in tech_list:\n",
    "        word1 = tech.replace(\"Pyspark\", \"PySpark\"\n",
    "                             ).replace(\"scikit-learn\", \"Scikit-learn\"\n",
    "                             ).replace(\"anglais\", \"Anglais\"\n",
    "                             ).replace(\"machine learning\", \"Machine Learning\"\n",
    "                             ).replace(\"Machine learning\", \"Machine Learning\"\n",
    "                             ).replace(\"Hadoop(HDFS)\", \"HDFS\"\n",
    "                             ).replace(\"NoSQ\", \"NoSQL\"\n",
    "                             ).replace(\"NoSQLL\", \"NoSQL\")\n",
    "        tech_list_processed.append(word1)\n",
    "    \n",
    "    return tech_list_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_list_processed = get_processed(tech_list)\n",
    "\n",
    "# processed dict\n",
    "tech_dict = {i:tech_list_processed.count(i) for i in tech_list_processed}\n",
    "# getting rid of phantom technologies names\n",
    "tech_dict.pop('', None)\n",
    "# sorted technologies dict in descending order\n",
    "sorted_dict = dict(sorted(tech_dict.items(), key=lambda item:item[1], reverse=True))\n",
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first 5 most used technologies\n",
    "max_5_values_tech_dict = sorted(tech_dict, key=tech_dict.get, reverse=True)[:5]\n",
    "max_5_values_tech_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultats :** \n",
    "\n",
    "Les 5 technologies les plus utilisées sont : \n",
    "- Python (6627 apparitions)\n",
    "- R      (4374 apparitions)\n",
    "- SQL    (2581 apparitions)\n",
    "- Java   (1719 apparitions)\n",
    "- Hadoop (1589 apparitions)\n",
    "\n",
    "Vu que l'on a traité au préalable les éléments de la liste `Technologies`, et qu'on les a stockés dans un **dictionnaire**, nous pouvons désormais tracer un histogramme des **5 technologies les plus utilisées**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.set_ylabel('Apparitions in work resumes')\n",
    "ax.set_xlabel('Technologies')\n",
    "ax.set_title('Most used technologies in data science jobs')\n",
    "\n",
    "# limit our dictionnary to the first max used technologies\n",
    "max_value = 5\n",
    "keys = dict(list(sorted_dict.items())[0:max_value]).keys()\n",
    "values = dict(list(sorted_dict.items())[0:max_value]).values()\n",
    "\n",
    "plt.bar(keys,values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Réaliser une méthode de clustering non supervisée de votre choix pour faire apparaître 2 clusters que vous jugerez pertinents. Donnez les caractéristiques de chacun des clusters.\n",
    "-  Justifier la performance de votre algorithme grace à une métrique.\n",
    "-  Interpréter votre resultat.  \n",
    "\n",
    "**Message** : (*Malheureusement je n'ai fait majoritairement que de l'algorithme supervisé et surtout qu'en Deep Learning, donc mes connaissances en ML et surtout dans le non supervisé sont très limitées voir quasi inexistantes... mais je vais quand même essayer de répondre à cette question.*)\n",
    "\n",
    "\n",
    "**Réponse :**  \n",
    "Pour cette question, nous allons utiliser les algorithmes de clustering présent dans `scikit-learn`. Nous jugeons pertinent qu'il y ait un rapport entre les colonnes `Metier` et `Experience`, nous allons donc faire apparaître deux clusters regroupant d'un côté les **ingénieurs débutants** et leurs métiers respectifs et de l'autre les **plus expérimentés** et leurs métiers respectifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement :\n",
    "\n",
    "Comme nous l'avons dit précédemment, il y a des valeurs d'expérience manquantes pour le travail de Data Engineer. Nous décidons alors de **reconstituer un nouveau dataset** (à partir du dataset *d'avant imputation* de la question 3) en remplissant les profils d'expérience du Data Engineer par une **valeur d'expérience médiane** (dans la colonne `Experience`) qui est arbitraire. \n",
    "\n",
    "Nous avons donc ainsi créé la nouvelle catégorie `Exp_label` mise à jour par rapport à celle que l'on avait auparavant (dans les précédentes questions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataset before applying categorical encoding (in the question 3)\n",
    "X_cluster_data = X_data.copy()\n",
    "#X_cluster_data\n",
    "# we're checking if there are any missing values \n",
    "print(X_cluster_data.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a des valeurs manquantes dans la colonne `Experience`, nous allons y remédier dans les blocs suivants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the column type to float type\n",
    "X_cluster_data['Experience'] = X_cluster_data['Experience'].astype(float)\n",
    "# mask for data architecte (cf question 4)\n",
    "mask4 = (X_data['Metier'] == 'Data architecte')\n",
    "\n",
    "print(f\"Are there any missing values for data architecte? : {X_cluster_data.loc[mask4, 'Experience'].isna().any()}\")\n",
    "print(f\"There are {X_cluster_data.loc[mask4, 'Experience'].isna().sum()} missing year of working experience values for the data architecte job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous effectuons le remplissage des valeurs manquantes à travers le bloc suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na values for data architecte experience in 'Experience' column \n",
    "X_cluster_data.loc[mask4, 'Experience'] = X_cluster_data.loc[mask4, 'Experience'].fillna(X_cluster_data['Experience'].median())\n",
    "\n",
    "# test if there are still na values \n",
    "print(f\"Are there any missing values for data architecte? : {X_cluster_data.loc[mask4, 'Experience'].isna().any()}\")\n",
    "print(f\"There are {X_cluster_data.loc[mask4, 'Experience'].isna().sum()} missing year of working experience values for the data architecte job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'y a désormais plus aucune valeurs manquantes dans nos données essentielles (`Experience` et `Exp_label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the new column using cut() method from pandas\n",
    "X_cluster_data['Exp_label'] = pd.cut(X_cluster_data['Experience'], 4, labels=['Débutant', 'Confirmé', 'Avancé', 'Expert'])\n",
    "X_cluster_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are still missing values\n",
    "print(f\"Are there any missing values for the 'Experience' column ? {X_cluster_data['Experience'].isna().any()}\")\n",
    "print(f\"Are there any missing values for the 'Exp_label' column ? {X_cluster_data['Exp_label'].isna().any()}\")\n",
    "# generate descriptive statistics of the 'Experience' column which is the only column with float values\n",
    "print(f\"\\nStatistical data of the 'Experience' column after filling missing values from data architecte jobs:\")\n",
    "X_cluster_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des colonnes pertinentes\n",
    "\n",
    "Nous récupérons donc les colonnes qui nous intéressent pour ce problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we define the clusters category columns\n",
    "clusters_category = pd.DataFrame(X_cluster_data, columns=['Experience', 'Metier'])\n",
    "# checking missing values\n",
    "print(clusters_category.isna().any())\n",
    "#clusters_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir appliquer l'algorithme non supervisé `KMeans`, il faut que les données de la colonne **`Metier`** soit au format labellisé (`float` ou `int`) pour permettre leur exploitation. Sachant que nous savons déjà que les valeurs dans cette colonne sont hiérarchisées (**cf. q6**), nous pouvons effectuer une méthode d'encodage directement sur la colonne **`Metier`** pour convertir les valeurs `str` en labels exploitables de type `int` prenant la valeur 0 ou 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the 'Metier' column type to categorical\n",
    "clusters_category['Metier'] = clusters_category['Metier'].astype('category')\n",
    "# Get list of categorical variables\n",
    "data_type_object = (clusters_category.dtypes == 'category')\n",
    "categorical_cols = list(data_type_object[data_type_object].index)\n",
    "\n",
    "# check the dataset\n",
    "print(f\"Categorical variables:\")\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc appliquer une méthode d'encodage appelée (**one-hot encoding**) sur la variable continue **`Metier`** pour pouvoir exploiter les valeurs de celle-ci dans le cadre de notre algorithme non supervisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "clusters_category = pd.DataFrame(OH_encoder.fit_transform(clusters_category[categorical_cols]))\n",
    "\n",
    "# get back the name of the columns\n",
    "clusters_category.columns = OH_encoder.get_feature_names()\n",
    "# One-hot encoding removed index; put it back\n",
    "clusters_category.index = X_cluster_data.index\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_clusters_category = X_cluster_data.drop(categorical_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_clusters_category = pd.concat([num_clusters_category, clusters_category], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding back the column Experience from the preprocessed data\n",
    "experience = X_cluster_data['Experience']\n",
    "clusters_category['Experience'] = experience\n",
    "\n",
    "# check the new dataset to work with and the features names\n",
    "print(f'Features names : \\n{OH_encoder.get_feature_names()} \\n')\n",
    "\n",
    "# checking the new columns after adding back the 'Experience' one\n",
    "clusters_category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application de la méthode de clustering KMeans\n",
    "\n",
    "Désormais, nous avons nos colonnes encodées et des valeurs exploitables pour la méthode de clustering `KMeans`. Nous allons donc essayer de la mettre en place et en dégager 2 clusters bien distinct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# get the features we want to apply KMeans\n",
    "clustering_features = [clustering_features for clustering_features in clusters_category.columns]\n",
    "X = clusters_category[clustering_features]\n",
    "#print(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying KMeans\n",
    "n_cluster = 2\n",
    "kmeans = KMeans(init=\"random\", n_clusters=n_cluster, random_state=0)\n",
    "y_kmeans = kmeans.fit(X.values)\n",
    "\n",
    "print(f'KMeans inertia : {kmeans.inertia_} \\n')\n",
    "print(f'KMeans cluster centers :\\n {kmeans.cluster_centers_} \\n')\n",
    "print(f'KMeans number of iteration to converge : {kmeans.n_iter_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons la métrique `silhouette_score` pour évaluer notre séparation en deux clusters de nos données contenues dans **X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# store kmeans labels\n",
    "label = kmeans.labels_\n",
    "\n",
    "# get the silhouette score of our model\n",
    "kmeans_silhouette = silhouette_score(X, label).round(3)\n",
    "print(f'For {n_cluster} clusters, the average Silhouette score is : {kmeans_silhouette}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur de `silhouette_score` varie **entre -1 et 1**, si le score est **de 1** alors les clusters sont denses et bien séparés entre eux. Une valeur du score proche **de 0** correspond à une différenciation pas assez prononcée entre les différents clusters (l'algorithme a du mal à les séparer en clusters bien distincts). Une valeur négative (entre **-1 et 0**) indique que les valeurs exploitées ne sont pas adéquates ou bien qu'elles ont été mal assignées aux clusters.  \n",
    "Le score obtenu est de **0.603**, ce qui n'est pas si mal même si l'on peut l'améliorer. Un score de silhouette compris entre **0.5 et 1** peut être considéré comme étant représentatif d'une bonne différenciation entre les clusters.\n",
    "\n",
    "**Message :**\n",
    "*Je ne sais pas si la réponse de la question 8 est juste, mais je vous propose quand même ma démarche pour utiliser au mieux cet algorithme non supervisé et comment j'ai fait pour parvenir à ce résultat*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Réaliser la prédiction des métiers manquants dans la base de données par l'algorithme de votre choix\n",
    "-  Justifier la performance de votre algorithme grace à une métrique.\n",
    "-  Interpréter votre resultat.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse :**\n",
    "\n",
    "Nous pouvons utiliser les données disponibles des autres colonnes pour pouvoir prédire les métiers manquants dans la colonne **`Metier`**, cependant il y a un gros mur qui se heurte à nous : **les données ne sont pas numériques**.  \n",
    "Deux choix s'offrent donc à nous, soit nous convertissons en numérique toutes les données pertinentes (utiles) pour pouvoir prédire quels types de métiers manquent (**en utilisant les méthodes d'encodage**) => *prétraitement des données long*, soit nous essayons de prédire les métiers manquants de la colonne **`Metier`** seulement avec l'aide de la colonne **`Experience`** qui est la seule contenant des valeurs numériques. La deuxième approche ne semble pas être assez efficace pour ce problème.  \n",
    "\n",
    "Compte tenu du dataset et des différentes catégories qui peuvent être pertinentes pour la détermination des métiers manquants, nous tentons **la première approche** car elle nous semble la plus efficace pour ce problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we're using our data preprocessed from question 8\n",
    "X_final_data = X_cluster_data.copy()\n",
    "# checking how many missing values the column 'Metier' have\n",
    "print(f\"Are there any missing values in the 'Metier' column ? : {X_final_data['Metier'].isna().any()}\")\n",
    "print(f\"There are {X_final_data['Metier'].isna().sum()} missing values in the 'Metier' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétaitement.\n",
    "\n",
    "Nous devons désormais garder seulement les variables pertinentes pour nos prédictions. Pour cela, nous allons étudier en détails les colonnes **`Technologies`**, **`Diplome`** et **`Exp_label`**, car elles semblent les plus pertinentes pour la prédiction des métiers manquants de la colonne **`Metier`**. \n",
    "\n",
    "Dans un premier temps, nous allons effectuer un encodage de la colonne **`Technologies`** pour pouvoir récupérer les technologies présentes dans les différents profils d'ingénieur data science de notre dataset. Nous reprenons les mêmes méthodes de prétraitement car il y a toujours les même soucis d'orthographe des technologies que pour la question 7, mais cette fois-ci, la fonction de processing d'orthographe sera optimisée pour garder **l'attribution des technologies au bon index**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process error names in our list \n",
    "def replace_words(tech_list):\n",
    "    \"\"\" Processing error in names without changing the lists dimensions \"\"\"\n",
    "    for i, lists in enumerate(tech_list):\n",
    "        for j, elem in enumerate(lists):\n",
    "            \n",
    "            if elem == 'Pyspark':\n",
    "                tech_list[i][j] = 'PySpark'\n",
    "            if elem == 'scikit-learn':\n",
    "                tech_list[i][j] = 'Scikit-learn'\n",
    "            if elem == 'anglais':\n",
    "                tech_list[i][j] = 'Anglais'                \n",
    "            if elem == 'machine learning':\n",
    "                tech_list[i][j] = 'Machine Learning'\n",
    "            if elem == 'Machine learning':\n",
    "                tech_list[i][j] = 'Machine Learning'\n",
    "            if elem == 'Hadoop(HDFS)':\n",
    "                tech_list[i][j] = 'HDFS'\n",
    "            if elem == 'NoSQ':\n",
    "                tech_list[i][j] = 'NoSQL'\n",
    "            if elem == 'NoSQLL':\n",
    "                tech_list[i][j] = 'NoSQL'   \n",
    "                \n",
    "    return tech_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process none values in our nested lists\n",
    "def list_processing(tech_list):\n",
    "    \"\"\" Processing none values \"\"\"\n",
    "    processed_tech_list = []    \n",
    "    for i, lists in enumerate(tech_list):\n",
    "        for j, elem in enumerate(lists):\n",
    "            \n",
    "            lists = filter(None,lists)\n",
    "            lists = list(lists)\n",
    "        processed_tech_list.append(lists)\n",
    "                \n",
    "    return processed_tech_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into a list and extract elements with a slash suffix and stored them in sublists\n",
    "final_raw_tech_list = X_final_data['Technologies'].tolist() \n",
    "final_raw_tech_list = extract_slash(final_raw_tech_list)\n",
    "\n",
    "# apply processing to our list of technologies\n",
    "final_tech_list = replace_words(final_raw_tech_list)\n",
    "final_tech_list = list_processing(final_tech_list)\n",
    "\n",
    "#checking the first values of our new processed nested-list\n",
    "final_tech_list[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de processing d'orthographe a modifié la plus part des noms de technologies de sorte à les harmoniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_data['Technologies'] = final_tech_list\n",
    "#convert back to object type\n",
    "X_final_data['Technologies'].convert_dtypes\n",
    "print(f\"Type of the column 'Technologies' : {X_final_data['Technologies'].dtypes}\\n\")\n",
    "# drop the columns we do not need \n",
    "X_final_data = X_final_data.drop(columns=['Entreprise', 'Ville', 'Experience'])\n",
    "X_final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding de la colonne \"Technologies\".\n",
    "\n",
    "Nous allons utiliser le module `MultiLabelBinarizer` de `scikit-learn` pour effectuer une méthode d'encodage en **one-hot encoding** des nested-lists contenues dans la variable continue **`Technologies`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# use the multilabelinarizer from scikit-learn\n",
    "mlb = MultiLabelBinarizer()\n",
    "label_tech_list = mlb.fit_transform(X_final_data['Technologies'])\n",
    "\n",
    "# our new dataset with technologies encoded\n",
    "X_final_label = pd.DataFrame(label_tech_list, columns=mlb.classes_)\n",
    "print(X_final_label.columns)\n",
    "X_final_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding des colonnes pertinentes pour la prédiction.\n",
    "\n",
    "Nous avons pu encoder les différentes technologies apparaissant dans chaque profil d'ingénieurs, nous avons fait un pas vers la possibilité de prédire les métiers manquants. Désormais, nous devons effectuer l'encodage des autres colonnes de notre dataset **X_final_data**, c'est à dire : **`Diplome`** et **`Exp_label`**.\n",
    "\n",
    "Nous avons le choix d'effectuer du **label encoding** ou bien du **one-hot encoding**. Nous observons une certaine hiérarchie entre les valeurs des deux colonnes restantes, nous pouvons donc affirmer que ce sont toutes des **valeurs ordinales** ou **hiérarchisées** :\n",
    "\n",
    "* `No diploma` (0) < `Bachelor` (1) < `Master` (2) < `Phd` (3).\n",
    "* `Débutant` (0) <  `Confirmé` (1) < `Avancé` (2) < `Expert` (3).\n",
    "\n",
    "Nous décidons donc d'effectuer une labellisation par la méthode **label encoding** pour ces deux colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column 'Technologies' for our convenience to manipulate object columns\n",
    "# the technologies column is \"list\" so it's unhashabme to convert\n",
    "X_final_data = X_final_data.drop(columns=['Technologies'])\n",
    "\n",
    "# Get list of categorical variables\n",
    "s_final_data = (X_final_data.dtypes == 'object')\n",
    "object_cols_final = list(s_final_data[s_final_data].index)\n",
    "\n",
    "#print(\"Object variables:\")\n",
    "print(f\"Type objects columns : {object_cols_final}\")\n",
    "#output : ['Metier', Diplome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all object columns to category type\n",
    "for col in object_cols_final:\n",
    "    X_final_data[col] = X_final_data[col].astype('category')\n",
    "\n",
    "s_category = (X_final_data.dtypes == 'category')\n",
    "category_cols_final = list(s_category[s_category].index)\n",
    "# check column types\n",
    "print(X_final_data.dtypes)\n",
    "X_final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos colonnes ont bien été converties en type catégorique. Avant de labelliser les colonnes restantes, nous devons vérifier l'ordre hiérarchique dans ces colonnes. Pour cela nous avons juste à vérifier cette ordre directement en affichant nos variables catégorielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking values order in categorical variables\n",
    "X_final_data['Exp_label'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous aurons donc comme ordre :\n",
    "* `Débutant` (0) <  `Confirmé` (1) < `Avancé` (2) < `Expert` (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store a new column with the label-encoded category Exp_label\n",
    "X_final_data['Exp_label_category'] = X_final_data['Exp_label'].cat.codes\n",
    "X_final_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous passons désormais à la remise en ordre hiérarchique des valeurs dans la colonne **`Diplome`**.\n",
    "\n",
    "* `No diploma` (0) < `Bachelor` (1) < `Master` (2) < `Phd` (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the values in order \n",
    "X_final_data[\"Diplome\"] = pd.Categorical(X_final_data[\"Diplome\"], categories=[\"No diploma\",\"Bachelor\",\"Master\",\"Phd\"], ordered=True)\n",
    "print(X_final_data[\"Diplome\"].head())\n",
    "\n",
    "# label encoding the column \"Diplome\" and stores it into a new column \"Dip_label\"\n",
    "X_final_data['Dip_label'] = X_final_data['Diplome'].cat.codes\n",
    "X_final_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding de la colonne \"Metier\".\n",
    "\n",
    "Nous avons désormais les colonnes nécessaires pour effectuer nos prédictions, néanmoins nous devons d'abord labelliser la colonne `Metier` sans pour autant labelliser ou déplacer les valeurs manquantes. \n",
    "\n",
    "Il y a 4 types de métiers différents, nous allons donc effectuer un **label-encoding** sans prendre en compte l'ordinalité entre les différents métiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing label encoding without touching the nans values\n",
    "X_merge_data = X_final_data.copy()\n",
    "\n",
    "# drop the column we do not need to use anymore\n",
    "X_merge_data = X_merge_data.drop(columns=['Diplome', 'Exp_label'])\n",
    "X_merge_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons alors commencer l'encodage des métiers tout en préservant les cases de métiers manquants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our 'Metier' column before encoding\n",
    "X_merge_data['Metier'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc les labels suivants pour les différents métiers : \n",
    "- `Data architecte` (0)\n",
    "- `Data engineer` (1)\n",
    "- `Data scientist` (2)\n",
    "- `Lead data scientist` (3)\n",
    "\n",
    "Nous effectuons donc le **label-encoding** de la colonne **`Metier`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label-encoding of the column 'Metier' (the only one of category type)\n",
    "X_merge_data_tmp = X_merge_data.astype(\"category\").apply(lambda elem: elem.cat.codes)\n",
    "# keep the na values \n",
    "X_merge_data = X_merge_data_tmp.where(~X_merge_data.isna(), X_merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are still any na values\n",
    "print(f\"Missing values : \\n\\n{X_merge_data.isna().sum()}\")\n",
    "# check our new dataset\n",
    "X_merge_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons réussi à encoder la colonne **`Metier`** tout en conservant les valeurs manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge des deux datasets encodés\n",
    "\n",
    "Nous avons désormais tous les éléments pour commencer nos prédictions, pour commencer nous allons donc fusionner les datasets encodés ensemble dans notre variable appelée **X_final**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = X_final_label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = X_merge_data['Metier']\n",
    "col2 = X_merge_data['Exp_label_category']\n",
    "col3 = X_merge_data['Dip_label']\n",
    "\n",
    "# insert the columns of X_merge_data to X_final_data\n",
    "X_final.insert(0, column='Metier', value=col1)\n",
    "X_final.insert(1, column='Exp_label_category', value=col2)\n",
    "X_final.insert(1, column='Dip_label', value=col3)\n",
    "\n",
    "# overview of the new dataset available\n",
    "X_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons explorer la corrélation entre les différentes colonnes à l'aide de la méthode **corr()** implémentée dans `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#X_final.corr('pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus la valeur de corrélation est proche de 1, plus la corrélation entre les deux colonnes concernées est forte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application de l'algorithme de prédiction.\n",
    "\n",
    "Nous avons finalement réussi à obtenir notre super dataset contenant des valeurs de types `float`, nous pouvons désormais utiliser un algorithme d'apprentissage machine pour effectuer nos prédictions. Nous nous tournons naturellement vers un algorithme de régression pour ce type de données numériques, néanmoins la nature de l'exercice veut que l'on puisse prédire les métiers manquants ceci peut-être aussi interprété comme un problème de **classification**. \n",
    "\n",
    "Nous pouvons essayer d'effectuer les prédictions en utilisant un algorithme de régression puis référer ces prédictions à nos métiers correspondants: \n",
    "\n",
    "- `Data architecte` (0)\n",
    "- `Data engineer` (1)\n",
    "- `Data scientist` (2)\n",
    "- `Lead data scientist` (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get back our data after many processing steps\n",
    "X_data = X_final.copy()\n",
    "X_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons d'abord sélectionner dans un premier temps, un panel de features susceptibles d'être les plus intéressantes pour les prédictions de la colonne **`Metier`**, parmi toutes les technologies présentes nous allons sélectionner les **20 plus utilisées**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting sum values of the first 20 most used technologies columns in X_data\n",
    "list_best_tech = X_data.iloc[:,2:-1].sum().sort_values(ascending=False)[:21] \n",
    "list_best_tech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on fait abstraction de la variable **`Exp_label_category`**, nous avons listé les 20 technologies les plus utilisées par les ingénieurs en data science.  \n",
    "Dès lors, nous vérifions s'il y a toujours les valeurs manquantes dans notre colonne **`Metier`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying missing values in the dataset\n",
    "print(f\"Are there any missing values in 'Metier' column ? : {X_data['Metier'].isna().any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce type de prédiction, nous allons nettoyer les valeurs manquantes dans la colonne **`Metier`**. Nous définissons notre modèle comme suit, en utilisant un algorithme d'apprentissage machine appelé **RandomForest**, celui-ci est disponible dans `scikit-learn` sous le nom de `RandomForestRegressor`. Nous utiliserons ici deux métriques de précision  **r2_score** et **mean squared error** pour comparer nos prédictions avec les valeurs *ground truth*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# filters nan values\n",
    "X_data = X_data.dropna(axis=0)\n",
    "# Choosing the column 'Metier' to be predicted\n",
    "y = X_data.Metier\n",
    "# get all the relevant features in the dataset for this prediction task\n",
    "data_features = ['Python', 'R', 'SQL','Java','Hadoop','Machine Learning',\n",
    "                 'Excel','C++','Spark','Linux','MongoDB','VBA','SAS',\n",
    "                 'Docker','Matlab', 'Hive','Tensorflow','Elasticsearch','Big data','Scikit-learn']\n",
    "X = X_data[data_features]\n",
    "\n",
    "# splitting data into training and validation data, for both features and target (randomly)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, random_state=0)\n",
    "print(f\"Training set shape : {X_train.shape}\")\n",
    "print(f\"Test set shape : {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model\n",
    "regression_forest_model = RandomForestRegressor(random_state=0)\n",
    "regression_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "metier_preds = regression_forest_model.predict(X_val)\n",
    "metier_preds = np.rint(metier_preds)\n",
    "\n",
    "# mean square error and r2_score\n",
    "mse = mean_squared_error(y_val, metier_preds)\n",
    "r2_score = r2_score(y_val, metier_preds)\n",
    "print(f\"Mean Square Error for our predictions : {mse:.3f}\")\n",
    "print(f\"r2_score for our predictions : {r2_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons des résultats plutôt satisfaisant, **l'erreur quadratique moyenne** (mse ~ 0.18) est correcte mais sur ce type de problème de regression celle-ci a tendance à traduire une légère imprécision sur les prédictions de l'algorithme. Tandis que le **coefficient de détermination** (r2_score ~ 0.81) se rapproche de sa meilleure valeur possible qui est de **1**, ce qui peut nous conforter dans notre idée que les scores obtenus sont satisfaisants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"10 first values of 'Metier' : \\n\\n{y_val[:10]}\")\n",
    "print(f\"\\n10 first predictions : {metier_preds[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous tentons ensuite d'améliorer notre algorithme, cette fois-ci au lieu de prendre **les 20 technologies les plus utilisées** en tant que features, nous allons prendre **TOUTES** les technologies contenues dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# filters nan values\n",
    "X_data = X_data.dropna(axis=0)\n",
    "# Choosing the column 'Metier' to be predicted\n",
    "y = X_data.Metier\n",
    "# get all the features in the dataset for this prediction task\n",
    "data_features = [col for col in X_data.columns]\n",
    "X = X_data[data_features]\n",
    "\n",
    "# splitting data into training and validation data, for both features and target (randomly)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, random_state=0)\n",
    "print(f\"Training set shape {X_train.shape}\")\n",
    "print(f\"Test set shape {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model\n",
    "regression_forest_model = RandomForestRegressor(random_state=0)\n",
    "regression_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "metier_preds = regression_forest_model.predict(X_val)\n",
    "metier_preds = np.rint(metier_preds)\n",
    "\n",
    "# mean square error and r2_score\n",
    "mse = mean_squared_error(y_val, metier_preds)\n",
    "r2_score = r2_score(y_val, metier_preds)\n",
    "print(f\"Mean Square Error for predictions : {mse}\")\n",
    "print(f\"r2_score : {r2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"10 first values of 'Metier' : \\n\\n{y_val[:10]}\")\n",
    "print(f\"\\n10 first predictions : {metier_preds[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En prenant toutes les colonnes en tant que features pour pouvoir prédire la colonne **`Metier`**, nous obtenons un score parfait avec une **erreur quadratique moyenne** nulle (mse = 0.0) ainsi qu'un **coefficient de détermination** maximal (r2_score = 1.0). Nous pouvons donc en conclure que **notre première approche** (cf. début question 9) est une très bonne approche pour ce problème. \n",
    "\n",
    "Les colonnes contenant les informations pertinentes pour la prédiction des profils de métiers dans la data science, sont celles des **technologies**. C'est grâce à elles et les informations qu'elles contiennent que nous pouvons **prédire quel métier dans la data science correspond en se basant sur leur panel de compétences techniques/technologiques**.\n",
    "\n",
    "**Message :** *Cependant, compte-tenu de la question posée, je ne suis pas sur que ma réponse soit la bonne car je n'ai pas \"prédit\" les valeurs manquantes mais plutôt essayer de prédire les métiers à partir des valeurs existantes et comparer mes prédictions aux ground truth. Libre à vous d'en juger !*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
